Perceptron

1.Files included
-Wang_perceptron.py: finished all TODDOs
-Readme.MD: test accuracy, feature weights, Experiment

2.Test Results
Run './Wang_perceptron.py', the result is as below:
The most suitable learning rate is: 0.03274549162877728 
The best number of iterations for that learning rate is: 52
Test accuracy: 0.8181066067840681
Feature weights (bias last): -0.1883363971171672 -0.12547678514685467 0.10320397181323464 0.11662727077938177 -0.0018074895787680845 0.10841949668978478 0.02097876277151285 0.06226470287237493 0.23369435716613451 0.04427531814620202 -0.03777287915011136 -0.25693288730599817 0.0 -0.22366417549636025 0.07512154355645437 0.03559803202217489 -0.003797198824670167 0.020952369492227507 -0.1123688863785051 0.03212379216904539 0.12062777070738033 0.002706671444041131 0.025330205412709528 0.005669500260972515 0.05722896168152766 -0.017215718624487494 -0.07756698174619296 0.19488998057587908 0.031792847675334926 0.10331801379198961 0.037759423239188475 0.1695035516236641 0.009805405644492629 -0.6793939667272137 -0.307776073138964 0.002706671444041131 0.03212379216904539 0.06289846194250023 0.11425771833320357 0.1697374783541637 -0.09509269657869661 -0.20895258544547624 -0.16910670805755207 0.03208079013955213 -0.12426337847631962 0.299807670814155 0.10915549013187213 -0.0041554237152335305 -0.08457323964982902 0.08931584329129102 0.25068466871480244 -0.030105799361832733 -0.03179960958265966 -0.019845328661814043 0.03012843834319802 -0.10937585940988236 -0.029706135803669112 -0.22385527935582533 0.22905910624948173 0.0 0.17442306770061022 -0.07133553797503353 -0.04627920719427438 -0.07507358686390267 -0.0857428721782307 0.008218707260657265 -0.03389349237212698 0.19525287678524145 -0.06445158407671123 -0.10271531583870779 -0.08998191374786921 -0.07330539021411256 -0.022484039036061068 -0.16486286547809176 0.06907343622791809 -0.1794248091275138 0.08363537987734024 -0.18117711851979684 0.0021785650831184222 -0.06747702379523687 0.03444981988957431 0.11623632809216737 0.1053536140904902 0.28886248125299224 0.12793807629498144 -0.13512828542470717 0.2404778467555062 -0.0846631425755665 0.0 -0.3222134861478498 0.14139465515215655 0.007795629338985963 0.06950793020161164 -0.13206975178895114 0.08153671428725415 -0.053876072983722795 -0.03389524862417502 0.0956678236029128 0.16441575728815733 0.062273030344565966 0.001042641676283318 -0.24399587937666223 0.0552716898335331 -0.06644460304253477 0.24138331647744837 -0.04636400381576565 -0.1715280351513329 -0.2935196884115203 0.4070309992763291 -0.05654538209238062 0.1362374327662919 -0.2765383233315547 -0.17366541813717679 0.19097476606519098 0.26883804368743947 0.20004165345931785 -0.22244245182196154 0.35754885503920236 0.09987280064879532 -0.13365719027914735 -0.31276159593849856 -0.13679018420421268 0.0 -0.09578942925017368


3. Experiments
3.1 Run './Wang_perceptron.py --nodev', the dev data will be used to avoid overfitting. The result is as below:
> Test accuracy: 0.81361541189\
> Feature weights (bias last): -6.0 -6.0 5.0 3.0 0.0 1.0 0.0 5.0 9.0 2.0 0.0 -8.0 0.0 -9.0 6.0 0.0 -2.0 1.0 -4.0 2.0 3.0 -1.0 1.0 -1.0 2.0 0.0 -3.0 7.0 1.0 3.0 3.0 7.0 -2.0 -22.0 -11.0 -1.0 2.0 1.0 5.0 6.0 -3.0 -8.0 -3.0 -1.0 -3.0 8.0 3.0 1.0 -1.0 3.0 10.0 -1.0 0.0 1.0 0.0 -5.0 -4.0 -5.0 7.0 0.0 5.0 0.0 -3.0 -1.0 -5.0 0.0 1.0 4.0 -3.0 -4.0 -2.0 -4.0 0.0 -8.0 4.0 -6.0 2.0 -7.0 0.0 -3.0 2.0 4.0 5.0 12.0 4.0 -7.0 7.0 -1.0 0.0 -9.0 6.0 -1.0 -1.0 -3.0 7.0 -2.0 0.0 3.0 5.0 3.0 3.0 -8.0 1.0 -6.0 7.0 0.0 -6.0 -8.0 12.0 0.0 2.0 -9.0 -6.0 4.0 7.0 7.0 -7.0 10.0 1.0 -4.0 -11.0 -2.0 0.0 -4.0

3.2 When use constant # of learning rate, the model converged when # of iteration is close to 52.  After convergence, the prediction accuracy is around 80%

3.3 when use constant # of iterations, the accuracy is around 80% andd does not change the learning rate.

